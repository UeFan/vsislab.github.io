
<html>
<title>Autonomous Robot Navigation Based on Multi-Camera Perception</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--Import Google Icon Font-->
      <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <!--Import materialize.css-->
      <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
      <!--Let browser know website is optimized for mobile-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>    

<body>
	<br />
<center><h4 class="black-text" style="padding-left: 210px; padding-right: 210px;">Autonomous Robot Navigation Based on Multi-Camera Perception</h4></center>
<center><h6 class="gray-text">Kunyan Zhu, Wei Chen, Wei Zhang, Ran Song, and Yibin Li</h6></center>
<p style="padding-left: 210px;"><b>Abstract</b></p>       
<p style="padding-left: 210px; padding-right: 210px; text-align:justify; text-justify:inter-ideograph;">
In this paper, we propose an autonomous system for robot navigation based on a specifically designed multicamera setup that takes advantage of the wide field of view to
obtain the state of the robot. Such system enables the robot to act properly by comprehensively estimating both its own
state and the high-level control commands extracted from the map. The visual information supplied by the left, middle and
right cameras is subsequently refined for multiple sub-tasks, including behavior reflex and scene perception. Our system
combines the mediated perception, the direct perception and the behavior reflex to reliably generate navigation indicators as well
as robot actions, which are then fed into the driving controller. Experiments in real-world environments demonstrates that our
system accomplishes both local path planning for obstacle avoidance and global goal-directed navigation.

</p>
<!--p style="padding-left: 210px;"><b>● Data: <a href="data.html" target="_blank">View our datasets</a></b></p-->
<center><img src="image/overview.jpg" width="35%">
<br> Overview of the proposed method.
</center>
<p style="padding-left: 210px;"><b>● Data & Code: </b>Coming soon!</p>
<p style="padding-left: 210px;"><b>● Video:</b></p>
<!--h5 style="padding-left: 210px;"><blockquote>Explicit method for robot imitation learning</blockquote></h5>
<div style="padding-left: 210px; padding-right: 210px;"><hr></div-->
<center>
 <iframe width="640" height="360" src="https://www.youtube.com/embed/QvHZ1bpvwno" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  <br><br>
  
</center>

<!--iframe width="640" height="359" src="https://www.youtube.com/embed/uaJXXVa31wU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe-->
<br><br>


<!--h5 style="padding-left: 210px;"><blockquote>Implicit method for robot imitation learning</blockquote></h5
Simulation and real-world experiments video of our implicit method for robot imitation learning
Real world experiments video of our explicit method for robot imitation learning
-->
<!--Import jQuery before materialize.js-->
      <script type="text/javascript" src="js/jquery-2.1.1.min.js"></script>
      <script type="text/javascript" src="js/materialize.min.js"></script>
</body>
</html>